---
title: "MPM-Project"
author: "Lorenz Isenegger, Philipp BÃ¤chler"
date: '2022-04-27'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(magrittr)
```


\newpage
# Load the data

Monitoring health data is getting very popular nowadays. With the help of smart watches and other wearable devices collecting and generating data is easier than ever and will likely keep evolving. An useful application would be, if we can predict certain illnesses and diseases - before these have a severe impact on the patient. We have decided to work on a data set, which holds data from patients which have had an stroke and control data from patients which have not had an stroke. Our goal would be to learn how to prepare such a data set, train different models make predictions with these and compare the outcomes.

The data set is from kaggle: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

```{r}
d.stroke <- read.csv("healthcare-dataset-stroke-data.csv", header=TRUE, stringsAsFactors=TRUE)
str(d.stroke)
```


If we compare the number of stroke occurrences and the number of observations we see that the data set is unbalanced. Only about 4.2% of all observations have a positive stroke outcome. If we would implement a model which returns always a negative answer (e.g. no stroke), our model would have an accuracy of 95.7%. However, the difficult and valuable task of such a problem is to predict the cases, where the patient will possibly have a stroke.

```{r}
nrow(d.stroke)
prop.table(table(d.stroke$stroke))
str(d.stroke$stroke)
```


```{r, fig.height=3}
barplot(c(sum(d.stroke$stroke==0), sum(d.stroke$stroke==1)), names.arg=c("!stroke", "stroke"), 
        main="Compare Groups 'stroke' and '!stroke'")
```


# Data Cleaning

```{r}
summary(d.stroke)
```


The variable **id** does not add any value to our models. So it is useless information and can be neglected.
```{r}
d.stroke <- subset(d.stroke, select=-id)
```


All variable names are written in lowercase except **Residence_type**. Let us keep the convention and rename this variable.
```{r}
d.stroke <- d.stroke %>% 
  rename(residence_type = Residence_type)
```


Checking the gender categories reveals three factors: Female, Male and Other. As there is only one observation with the factor **Other**, we should simplify our model and remove this observation. As this is a medical data set, the biological gender is higher valued than the identified gender. this by all means should not be taken as offense against non-binary people. 
```{r}
d.stroke <- d.stroke[c(d.stroke$gender != "Other"),]
```

The values for variable **bmi** are initially interpreted as factors. Let us change these to numeric values.
```{r, warning=FALSE}
d.stroke <- d.stroke %>%
  group_by(bmi) %>%
  mutate(bmi = as.numeric(levels(bmi))[bmi]) 
```

We have only missing values for the variable **bmi**. As we have only 201 missing values compared to total 5110 observations it would be reasonable to drop these observations.
```{r}
d.stroke <- d.stroke %>%
  drop_na()
```

The variable **age** can be interpreted as count variable, at least from values of age > 2. Values < 2 are represented as decimal fractions. this contradicts the majority of rows, so we cater for this with coercing these ages to integers of value 0, 1, or 2
```{r}
d.stroke <- d.stroke %>%
  mutate(age.int = as.integer(age)) %>%
  select(., -age)
```




```{r}
summary(d.stroke)
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$bmi, main="Histogram of BMI")
d.stroke$bmi_norm <- log(d.stroke$bmi)/max(log(d.stroke$bmi))
hist(d.stroke$bmi_norm, main="Histogram of log(BMI)")
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$age.int)
d.stroke$age_norm <- d.stroke$age.int/max(d.stroke$age.int)
hist(d.stroke$age_norm)
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$avg_glucose_level)
d.stroke$avg_glucose_level_norm <- d.stroke$avg_glucose_level / max(d.stroke$avg_glucose_level)
hist(d.stroke$avg_glucose_level_norm)
```

Datatypes:
```{r}
d.stroke$stroke <- as.factor(d.stroke$stroke)
str(d.stroke)
```

\newpage
# Graphical Analysis
```{r}
qplot(y = bmi, x = age.int,
      data = d.stroke,
      facets = ~ gender,
      col = as.factor(stroke))
```

```{r}
boxplot(age.int ~ stroke, data = d.stroke,
        main = "Influence of Age on Stroke Probability",
        ylab = "age")
```

```{r}
boxplot(avg_glucose_level ~ stroke, data = d.stroke,
        main = "Influence of Glucose Level on Stroke Probability",
        ylab = "avg_glucose_level")
```

```{r}
boxplot(bmi ~ stroke, data = d.stroke,
        main = "Influence of BMI on Stroke Probability",
        ylab = "bmi")
```

```{r}
boxplot(age.int ~ smoking_status, data = d.stroke,
        main = "Age vs. Smoking Status",
        ylab = "age")
```


```{r}
plot(d.stroke$bmi_norm, d.stroke$avg_glucose_level_norm, 
     col="cornflowerblue", pch=20)
points(d.stroke$bmi_norm[d.stroke$stroke==1], 
       d.stroke$avg_glucose_level_norm[d.stroke$stroke==1], 
       col="firebrick", pch=20)
```


\newpage
# Train / Test / Validation Split
As we have an unbalanced data set, we must first apply some balancing algorithm. For a first approach, oversampling should be sufficient. However, to make sure, that we do not induce any errors with the oversampling algorithm, the first thing we should do is to split off some validation data. After this, oversampling can be applied to the remaining data set. For training the model, we can split now the over-sampled set into training an testing data. 

1. Split data into training and validation set (train:90% / valid:10%)
2. Apply Oversampling to the training data set
3. Split training data set into training and testing set (train:80% / test:20%)

```{r}
table(d.stroke$stroke)
```


## Validation Split
```{r}
set.seed(42)
split1<- sample(c(rep(0, round(0.9 * nrow(d.stroke))), rep(1, round(0.1 * nrow(d.stroke)))))
table(split1)
```

```{r}
d.train <- d.stroke[split1 == 0, ]
d.valid <- d.stroke[split1== 1, ]
dim(d.train)
dim(d.valid)
```


## Oversampling
```{r}
#install.packages("ROSE")
library(ROSE)
```

```{r}
table(d.train$stroke)
```

```{r}
set.seed(42)
d.train <- ovun.sample(stroke ~ ., data=d.train, method="over", N=2*4233)$data
table(d.train$stroke)
```

```{r}
str(d.train)
```


## Train / Test Split
```{r}
set.seed(42)
split2<- sample(c(rep(0, 0.8 * nrow(d.train)), rep(1, 0.2 * nrow(d.train))))
d.test <- d.train[split2== 1, ]
d.train <- d.train[split2 == 0, ]
dim(d.train)
dim(d.test)
```


\newpage
# 2. Generalised Linear Model with family set to Poisson.
GLM with Poisson family does not make sense on our Stroke prediction, as Poisson distribution is used for modeling Count data response variable.Stroke is binary data, so Poisson distribution does not work.
Therefore we use a glm with a binomial distribution to fit stroke, as described in the next section.

To fit a glm with poisson distribution we need count data. The only count data variable in the data is 'age' > 2.0. Children aged < 2 are represented by decimal fractionals.. then we fit a glm with Poisson distribution with response variable age. 

```{r}

d.glm.pois <- glm(age.int ~ .-bmi-avg_glucose_level, data=d.train, family="poisson")
summary(d.glm.pois)
```
Now we see that the dispersion parameter is set to 1 because the use of the Poisson family. However, the Residual deviance differs greatly from the degrees of freedom (df) of the model. this counts as underdisperson, or rather lower increase in variance of the count data than anticipated with the use of a poisson distribution. 
We can account for this whit the use of a Quasipoisson distribution in the glm family

```{r}
# glm wiht Quasipoisson distribution 
d.glm.pois <- glm(age.int ~ .-bmi-avg_glucose_level, data=d.train, family="quasipoisson")
summary(d.glm.pois)
```
The Dispersion factor is now taken to be 0.336 instead of 1. This leads to some change in the p-values of the predictors. Indeed, The predictor 'hypertension' now is considered significant (although not highly) according to its p-value, whereas before it was not considered significant. 
Even more so 'heart_disease' becomes now highly significant. 
similarly the 'smoking_status' predictor gains significance, especially on the levels 'smokes' and 'Unknown'. 
Also 'bmi' in it's non-normalized form gets highly significant, but that was expected before through the use of the balanced normalized 'bmi_norm' predictor 

```{r}
pred.glm.pois <- predict(d.glm.pois, d.test)
plot(pred.glm.pois, d.test$age.int)
abline(range(pred.glm.pois))
```

\newpage
# 3. Generalised Linear Model with family set to Binomial

As described before, if stroke prediction is the goal of the modelling, a glm with the binomial distribution has to be used. 
aagain, we exclude any correlated variables, such as 'bmi' and 'avg_glucose_level' and rather use the normalized varesions of these.

```{r}
d.glm.bin <- glm(stroke ~ .-bmi-avg_glucose_level, data=d.train, family="binomial")
summary(d.glm.bin)
```
The model sees again significant contributions from 'hypertension', 'work_type', 'smoking_status' as well as the 'avg_glucose_level_norm' variables. 
Note that the stroke prediction has a much lower Residual Deviance than the age prediction whit the poisson distribution used before. However, still a small underdispersion is notable.

```{r}
d.glm.bin <- glm(stroke ~ .-age.int-bmi-avg_glucose_level, data=d.train, family="quasibinomial")
summary(d.glm.bin)
```

Now on to the prediction. 
```{r}
pred.glm.bin <- predict(d.glm.bin, d.test)
plot(pred.glm.bin, d.test$stroke)
```



```{r}
d.glm <- glm(stroke ~ bmi_norm + age_norm + avg_glucose_level_norm , data=d.train, family="binomial")
summary(d.glm)
```

```{r}
pred.glm.bin <- predict(d.glm, d.test)
plot(pred.glm.bin, d.test$stroke)
```



\newpage
# 4. Generalised Additive Model

Now a GAM model should be fit on stroke. again, the family has tp be specified as 'binomial' or 'quasi-binomial', as the response variable is binary data
```{r}
library("mgcv")

d.gam <- d.train %$%
  gam(stroke ~ age_norm + hypertension + heart_disease + ever_married + work_type 
      + residence_type + avg_glucose_level_norm +bmi_norm + smoking_status, 
      family = "binomial")

d.gam

# summary(d.gam)

```
The GAM model advices 

```{r}
pred.gam.bin <- predict(d.gam, d.test)
plot(pred.gam.bin, d.test$stroke)
table(pred.gam.bin, d.test$stroke)
```

\newpage
# 5. Support Vector Machine

```{r}
library(e1071)
library(caret)
```

For the training of the SVM we use the normalized values of avg_glucose_level, bmi, and age. Therefore, we drop the columns where these values are not normalized. We choose the kernel as linear and a cost value of 5, as these values returned a reasonable result. In the chapter *7.* we investigate different settings and models with the use of cross validation. We have here a SVM with two classes 1:"stroke" and 0:"no stroke" and a total of 3529 support vectors, which seems to be a lot, comparing to the examples covered in class. 


```{r}
set.seed(42)
d.svm <- svm(stroke ~ . - avg_glucose_level - bmi - age.int, data=d.train, 
             kernel = "linear", type="C-classification", cost = 5)
summary(d.svm)
```

If we predict the values for the train data set we get an overall accuracy of 77.6%. This value is relatively low in comparison, if the model would always just return "no stroke", the accuracy would be at 95%. However, the sensitivity is at 81.5% which means, that we are able to predict most of the positive cases.

```{r}
test_pred <- predict(d.svm, d.test)
conf_matrix <- confusionMatrix(as.factor(test_pred), d.test$stroke, positive="1")
conf_matrix
```

The problem with our training data is, that we have used oversampling for balancing the data set. Therefore, the test data set also contains values from the train data set and this means, that our algorithm might have seen the test data during the training period. It is therefore important to compare check the model with a separate validation set. With the validation data set the accuracy is almost at the same level as with the training data - 74.3%. However, the sensitivity is here a bit higher with 91.7% - this might just be luck and will be checked in chapter 7.

```{r}
valid_pred <- predict(d.svm, d.valid)
conf_matrix <- confusionMatrix(as.factor(valid_pred), d.valid$stroke, positive="1")
conf_matrix
```


\newpage
# 6. Neural Network
```{r}
library(nnet)
```

The neuronal network which we have trained consists of an input layer with 16 nodes, one single hidden layer with 20 nodes and an output layer with a single node. This leads to a total of 361 weights. Also on this model we make use of the previously normalized variables avg_glucose_level, bmi, age. The parameters were chosen by trial and error to achieve the best result. We will later in this chapter see, that this values might be improved and in chapter 7, we will compare different models. The parameter *size* correspondents to the number of nodes which are in the single hidden layer. The parameter *maxit* is the number of iterations after which the algorithm will stop to train unless it converged before.


```{r, results=FALSE}
set.seed(42)
d.net <- nnet(stroke ~ . - avg_glucose_level - bmi - age.int, data = d.train, size=20, 
              maxit=10000, rang=0.1, decay=5e-3, MaxNWts = 20000)
d.net
```

If we evaluate the model with the test set, we can predict 100% of the true positive "stroke". This would mean that the specificity for this model would be 100%, which seems to be almost impossible. This might be the result of leaking test data into the train data set and chances of over fitting are high.

```{r}
pred <- predict(d.net, d.test, type="class")
cm_nn <- table(pred=pred, true=d.test$stroke)
cm_nn
```

If we evaluate the model with the validation set, we can see, that the model is not as near as good, as we expected. Only 10 out of 28 positive cases were predicted correctly. This huge difference between the test and validation outcome might be the result of overfitting the model. It might be that with the 20 nodes in the hidden layer the 361 weights, we have too many parameters to describe the model.

```{r}
pred <- predict(d.net, d.valid, type="class")
cm_nn <- table(pred=pred, true=d.valid$stroke)
cm_nn
```

The confusion matrix gives us more information. The accuracy lays at 86.9% and is slightly better than the result of the previously trained SVM. However, if we compare the sensitivity, which is more relevant for this project, we only receive 41.7% which is a much worse than the result of the SVM. This might be the result of over fitting the model.

```{r}
conf_matrix <- confusionMatrix(as.factor(pred), d.valid$stroke, positive="1")
conf_matrix
```



\newpage
# 7. Cross Validation on SVM and Neural Network

To be sure which model is better suited for predicting the strokes we have implemented a cross validation. We create a test/train split randomly, train and evaluate the models on these data sets and compare the sensitivities. This should answer if we had just luck or bad luck with the models trained in chapter 6 and 7.

```{r}
sens.svm.lin <- c()
sens.svm.rad <- c()
sens.nnet.10 <- c()
sens.nnet.15 <- c()

set.seed(42)
for(i in 1:10){
  
  ## 1) Create train test split
  split1 <- sample(c(rep(0, round(0.8 * nrow(d.stroke))), 
                     rep(1, round(0.2 * nrow(d.stroke)))))
  data.train <- d.stroke[split1 == 0, ]
  data.test <- d.stroke[split1== 1, ]
  
  ## 2) Apply oversampling
  data.train <- ovun.sample(stroke ~ ., data=data.train, method="over", 
                            N=2*nrow(data.train))$data
  
  ## 3) Train SVM
  model.svm.lin <- svm(stroke ~ . - avg_glucose_level - bmi - age.int, data=d.train, 
                       kernel = "linear", type="C-classification", cost = 5)
  model.svm.rad <- svm(stroke ~ . - avg_glucose_level - bmi - age.int, data=d.train, 
                       kernel = "radial", type="C-classification", cost = 5)
  
  ## 3) Train neuronal network
  model.nnet.10 <- nnet(stroke ~ . - avg_glucose_level - bmi - age.int, 
                        data = data.train, size=10, maxit=10000, rang=0.1, 
                        decay=5e-3, MaxNWts = 20000, trace=FALSE)
  model.nnet.15 <- nnet(stroke ~ . - avg_glucose_level - bmi - age.int, 
                        data = data.train, size=15, maxit=10000, rang=0.1, 
                        decay=5e-3, MaxNWts = 20000, trace=FALSE)
  
  ## 4) Prediction on test data
  model.svm.lin.pred <- predict(model.svm.lin, data.test, type="class")
  model.svm.rad.pred <- predict(model.svm.rad, data.test, type="class")
  model.nnet.10.pred <- predict(model.nnet.10, data.test, type="class")
  model.nnet.15.pred <- predict(model.nnet.15, data.test, type="class")
  
  ## 5) Evaluation
  conf_matrix <- confusionMatrix(as.factor(model.svm.lin.pred), 
                                 data.test$stroke, positive="1")
  sens.svm.lin[i] <- conf_matrix$byClass["Sensitivity"]
  
  conf_matrix <- confusionMatrix(as.factor(model.svm.rad.pred), 
                                 data.test$stroke, positive="1")
  sens.svm.rad[i] <- conf_matrix$byClass["Sensitivity"]
  
  conf_matrix <- confusionMatrix(as.factor(model.nnet.10.pred), 
                                 data.test$stroke, positive="1")
  sens.nnet.10[i] <- conf_matrix$byClass["Sensitivity"]
  
  conf_matrix <- confusionMatrix(as.factor(model.nnet.15.pred), 
                                 data.test$stroke, positive="1")
  sens.nnet.15[i] <- conf_matrix$byClass["Sensitivity"]
}
```


```{r}
sens.svm.lin
sens.svm.rad
sens.nnet.10
sens.nnet.15
```

```{r}
boxplot(sens.svm.lin, sens.svm.rad, sens.nnet.10, sens.nnet.15, 
        names=c("svm.lin", "svm.rad", "nnet.10", "nnet.15"))
```

















