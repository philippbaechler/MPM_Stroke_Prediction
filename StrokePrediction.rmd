---
title: "MPM-Project"
author: "Lorenz Isenegger, Philipp BÃ¤chler"
date: '2022-04-27'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(magrittr)
```


\newpage
# Load the data

Monitoring health data is getting very popular nowadays. With the help of smart watches and other wearable devices collecting and generating data is easier than ever and will likely keep evolving. An useful application would be, if we can predict certain illnesses and diseases - before these have a severe impact on the patient. We have decided to work on a data set, which holds data from patients which have had an stroke and control data from patients which have not had an stroke. Our goal would be to learn how to prepare such a data set, train different models make predictions with these and compare the outcomes.

The data set is from kaggle: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

```{r}
d.stroke <- read.csv("healthcare-dataset-stroke-data.csv", header=TRUE, stringsAsFactors=TRUE)
str(d.stroke)
```


If we compare the number of stroke occurrences and the number of observations we see that the data set is unbalanced. Only about 4.2% of all observations have a positive stroke outcome. If we would implement a model which returns always a negative answer (e.g. no stroke), our model would have an accuracy of 95.7%. However, the difficult and valuable task of such a problem is to predict the cases, where the patient will possibly have a stroke.

```{r}
nrow(d.stroke)
prop.table(table(d.stroke$stroke))
str(d.stroke$stroke)
```


```{r, fig.height=3}
barplot(c(sum(d.stroke$stroke==0), sum(d.stroke$stroke==1)), names.arg=c("!stroke", "stroke"), main="Compare Groups 'stroke' and '!stroke'")
# d.stroke %>%
#   filter(.,sum(stroke == 0)) %>%
#   ggplot() +
#   geom_bar()

ggplot(data = d.stroke, aes(x = factor(stroke))) +
  geom_bar() +
  labs(x="stroke or !stroke", y = "n")
  
  
```


# Data Cleaning

```{r}
summary(d.stroke)
```


The variable **id** does not add any value to our models. So it is useless information and can be neglected.
```{r}
d.stroke <- subset(d.stroke, select=-id)
```


All variable names are written in lowercase except **Residence_type**. Let us keep the convention and rename this variable.
```{r}
d.stroke <- d.stroke %>% 
  rename(residence_type = Residence_type)
```


Checking the gender categories reveals three factors: Female, Male and Other. As there is only one observation with the factor **Other**, we should simplify our model and remove this observation. As this is a medical data set, the biological gender is higher valued than the identified gender. this by all means should not be taken as offense against non-binary people. 
```{r}
d.stroke <- d.stroke[c(d.stroke$gender != "Other"),]
```

The values for variable **bmi** are initially interpreted as factors. Let us change these to numeric values.
```{r, warning=FALSE}
# d.stroke$bmi <- as.numeric(levels(d.stroke$bmi))[d.stroke$bmi]

d.stroke <- d.stroke %>%
  group_by(bmi) %>%
  mutate(bmi = as.numeric(levels(bmi))[bmi]) 
```

We have only missing values for the variable **bmi**. As we have only 201 missing values compared to total 5110 observations it would be reasonable to drop these observations.
```{r}
# mean(as.numeric(as.character(d.stroke$bmi)), na.rm = TRUE)

d.stroke <- d.stroke %>%
  drop_na()
```

The variable **age** can be interpreted as count variable, at least from values of age > 2. Values < 2 are represented as decimal fractions. this contradicts the majority of rows, so we cater for this with coercing these ages to integers of value 0, 1, or 2
```{r}
d.stroke <- d.stroke %>%
  mutate(age.int = as.integer(age)) %>%
  select(., -age)
```




```{r}
summary(d.stroke)
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$bmi, main="Histogram of BMI")
d.stroke$bmi_norm <- log(d.stroke$bmi)/max(log(d.stroke$bmi))
hist(d.stroke$bmi_norm, main="Histogram of log(BMI)")
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$age.int)
d.stroke$age_norm <- d.stroke$age.int/max(d.stroke$age.int)
hist(d.stroke$age_norm)
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$avg_glucose_level)
d.stroke$avg_glucose_level_norm <- d.stroke$avg_glucose_level/max(d.stroke$avg_glucose_level)
hist(d.stroke$avg_glucose_level_norm)
```

Datatypes:
```{r}
#d.stroke$gender <- as.factor(d.stroke$gender)
#d.stroke$hypertension <- as.factor(d.stroke$hypertension)
#d.stroke$heart_disease <- as.factor(d.stroke$heart_disease)
#d.stroke$ever_married <- as.factor(d.stroke$ever_married)
#d.stroke$work_type <- as.factor(d.stroke$work_type)
#d.stroke$residence_type <- as.factor(d.stroke$residence_type)
#d.stroke$smoking_status <- as.factor(d.stroke$smoking_status)
d.stroke$stroke <- as.factor(d.stroke$stroke)
str(d.stroke)
```

\newpage
# Graphical Analysis
```{r}
qplot(y = bmi, x = age.int,
      data = d.stroke,
      facets = ~ gender,
      col = as.factor(stroke))
```

```{r}
boxplot(age.int ~ stroke, data = d.stroke,
        main = "Influence of Age on Stroke Probability",
        ylab = "age")
```

```{r}
boxplot(avg_glucose_level ~ stroke, data = d.stroke,
        main = "Influence of Glucose Level on Stroke Probability",
        ylab = "avg_glucose_level")
```

```{r}
boxplot(bmi ~ stroke, data = d.stroke,
        main = "Influence of BMI on Stroke Probability",
        ylab = "bmi")
```

```{r}
boxplot(age.int ~ smoking_status, data = d.stroke,
        main = "Age vs. Smoking Status",
        ylab = "age")
```


```{r}
plot(d.stroke$bmi_norm, d.stroke$avg_glucose_level_norm, col="cornflowerblue", pch=20)
points(d.stroke$bmi_norm[d.stroke$stroke==1], d.stroke$avg_glucose_level_norm[d.stroke$stroke==1], col="firebrick", pch=20)
```


\newpage
# Train / Test / Validation Split
As we have an unbalanced data set, we must first apply some balancing algorithm. For a first approach, oversampling should be sufficient. However, to make sure, that we do not induce any errors with the oversampling algorithm, the first thing we should do is to split off some validation data. After this, oversampling can be applied to the remaining data set. For training the model, we can split now the over-sampled set into training an testing data. 

1. Split data into training and validation set (train:90% / valid:10%)
2. Apply Oversampling to the training data set
3. Split training data set into training and testing set (train:80% / test:20%)

```{r}
table(d.stroke$stroke)
```


## Validation Split
```{r}
set.seed(42)
split1<- sample(c(rep(0, round(0.9 * nrow(d.stroke))), rep(1, round(0.1 * nrow(d.stroke)))))
table(split1)
```

```{r}
d.train <- d.stroke[split1 == 0, ]
d.valid <- d.stroke[split1== 1, ]
dim(d.train)
dim(d.valid)
```


## Oversampling
```{r}
#install.packages("ROSE")
library(ROSE)
```

```{r}
table(d.train$stroke)
```

```{r}
set.seed(42)
d.train <- ovun.sample(stroke ~ ., data=d.train, method="over", N=2*4233)$data
table(d.train$stroke)
```

```{r}
str(d.train)
```


## Train / Test Split
```{r}
set.seed(42)
split2<- sample(c(rep(0, 0.8 * nrow(d.train)), rep(1, 0.2 * nrow(d.train))))
d.test <- d.train[split2== 1, ]
d.train <- d.train[split2 == 0, ]
dim(d.train)
dim(d.test)
```


\newpage
# 1. Linear Model

```{r}
#d.lm <- lm(stroke ~ gender + age.int + avg_glucose_level + log(bmi), data=d.train)
#summary(d.lm)
```

```{r}
#d.lm <- lm(stroke ~ age.int + gender * age.int, data=d.train, )
#summary(d.lm)
```

```{r}
#d.lm <- lm(stroke ~ . - id, data=d.train, )
#summary(d.lm)
```

```{r}
#confint(d.lm)
```


\newpage
# 2. Generalised Linear Model with family set to Poisson.
GLM with Poisson family does not make sense on our Stroke prediction, as Poisson distribution is used for modeling Count data response variable.Stroke is binary data, so Poisson distribution does not work.
Therefore we use a glm with a binomial distribution to fit stroke, as described in the next section.

To fit a glm with poisson distribution we need count data. The only count data variable in the data is 'age' > 2.0. Children aged < 2 are represented by decimal fractionals.. then we fit a glm with Poisson distribution with response variable age. 

```{r}

d.glm.pois <- glm(age.int ~ .-bmi-avg_glucose_level, data=d.train, family="poisson")
summary(d.glm.pois)
```
Now we see that the dispersion parameter is set to 1 because the use of the Poisson family. However, the Residual deviance differs greatly from the degrees of freedom (df) of the model. this counts as underdisperson, or rather lower increase in variance of the count data than anticipated with the use of a poisson distribution. 
We can account for this whit the use of a Quasipoisson distribution in the glm family

```{r}
# glm wiht Quasipoisson distribution 
d.glm.pois <- glm(age.int ~ .-bmi-avg_glucose_level, data=d.train, family="quasipoisson")
summary(d.glm.pois)
```
The Dispersion factor is now taken to be 0.336 instead of 1. This leads to some change in the p-values of the predictors. Indeed, The predictor 'hypertension' now is considered significant (although not highly) according to its p-value, whereas before it was not considered significant. 
Even more so 'heart_disease' becomes now highly significant. 
similarly the 'smoking_status' predictor gains significance, especially on the levels 'smokes' and 'Unknown'. 
Also 'bmi' in it's non-normalized form gets highly significant, but that was expected before through the use of the balanced normalized 'bmi_norm' predictor 

```{r}
pred.glm.pois <- predict(d.glm.pois, d.test)
plot(pred.glm.pois, d.test$age.int)
abline(range(pred.glm.pois))
```

\newpage
# 3. Generalised Linear Model with family set to Binomial

As described before, if stroke prediction is the goal of the modelling, a glm with the binomial distribution has to be used. 
aagain, we exclude any correlated variables, such as 'bmi' and 'avg_glucose_level' and rather use the normalized varesions of these.

```{r}
d.glm.bin <- glm(stroke ~ .-bmi-avg_glucose_level, data=d.train, family="binomial")
summary(d.glm.bin)
```
The model sees again significant contributions from 'hypertension', 'work_type', 'smoking_status' as well as the 'avg_glucose_level_norm' variables. 
Note that the stroke prediction has a much lower Residual Deviance than the age prediction whit the poisson distribution used before. However, still a small underdispersion is notable.

```{r}
d.glm.bin <- glm(stroke ~ .-age.int-bmi-avg_glucose_level, data=d.train, family="quasibinomial")
summary(d.glm.bin)
```

Now on to the prediction. 
```{r}
pred.glm.bin <- predict(d.glm.bin, d.test)
plot(pred.glm.bin, d.test$stroke)
```



```{r}
d.glm <- glm(stroke ~ bmi_norm + age_norm + avg_glucose_level_norm , data=d.train, family="binomial")
summary(d.glm)
```

```{r}
pred.glm.bin <- predict(d.glm, d.test)
plot(pred.glm.bin, d.test$stroke)
```



\newpage
# 4. Generalised Additive Model

Now a GAM model should be fit on stroke. again, the family has tp be specified as 'binomial' or 'quasi-binomial', as the response variable is binary data
```{r}
library("mgcv")

d.gam <- d.train %$%
  gam(stroke ~ age_norm + hypertension + heart_disease + ever_married +work_type +residence_type + avg_glucose_level_norm +bmi_norm + smoking_status, family = "binomial")

d.gam

# summary(d.gam)

```
The GAM model advices 

```{r}
pred.gam.bin <- predict(d.gam, d.test)
plot(pred.gam.bin, d.test$stroke)
table(pred.gam.bin, d.test$stroke)
```

\newpage
# 5. Support Vector Machine

```{r}
library(e1071)
library(caret)
```


```{r}
set.seed(42)
d.svm <- svm(stroke ~ . - avg_glucose_level - bmi, data=d.train, kernel = "linear", type="C-classification", cost = 10)
summary(d.svm)
```


```{r}
test_pred <- predict(d.svm, d.test)
table(test_pred, d.test$stroke)
```

```{r}
valid_pred <- predict(d.svm, d.valid)
table(valid_pred, d.valid$stroke)
```



\newpage
# 6. Neural Network
```{r}
library(nnet)
```

```{r, results=FALSE}
set.seed(42)
d.net <- nnet(stroke ~ ., data = d.train, size=20, maxit=10000, rang=0.1, decay=5e-3, MaxNWts = 20000)
d.net
```

```{r}
pred <- predict(d.net, d.test, type="class")
cm_nn <- table(pred=pred, true=d.test$stroke)
cm_nn
```

```{r}
pred <- predict(d.net, d.valid, type="class")
cm_nn <- table(pred=pred, true=d.valid$stroke)
cm_nn
```

```{r}
conf_matrix <- confusionMatrix(as.factor(pred), d.valid$stroke, positive="1")
conf_matrix
```


\newpage
# 7. Optimisation Problem















