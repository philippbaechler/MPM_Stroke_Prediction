---
title: "MPM-Project"
author: "Lorenz Isenegger, Philipp BÃ¤chler"
date: '2022-04-27'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(magrittr)
```


\newpage
# Load the data

Monitoring health data is getting very popular nowadays. With the help of smart watches and other wearable devices collecting and generating data is easier than ever and will likely keep evolving. An useful application would be, if we can predict certain illnesses and diseases - before these have a severe impact on the patient. We have decided to work on a data set, which holds data from patients which have had an stroke and control data from patients which have not had an stroke. Our goal would be to learn how to prepare such a data set, train different models make predictions with these and compare the outcomes.

The data set is from kaggle: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

```{r}
d.stroke <- read.csv("healthcare-dataset-stroke-data.csv", header=TRUE, stringsAsFactors=TRUE)
str(d.stroke)
```


If we compare the number of stroke occurrences and the number of observations we see that the data set is unbalanced. Only about 4.2% of all observations have a positive stroke outcome. If we would implement a model which returns always a negative answer (e.g. no stroke), our model would have an accuracy of 95.7%. However, the difficult and valuable task of such a problem is to predict the cases, where the patient will possibly have a stroke.

```{r}
nrow(d.stroke)
prop.table(table(d.stroke$stroke))
str(d.stroke$stroke)
```


```{r, fig.height=3}
barplot(c(sum(d.stroke$stroke==0), sum(d.stroke$stroke==1)), names.arg=c("!stroke", "stroke"), main="Compare Groups 'stroke' and '!stroke'")
# d.stroke %>%
#   filter(.,sum(stroke == 0)) %>%
#   ggplot() +
#   geom_bar()

ggplot(data = d.stroke, aes(x = factor(stroke))) +
  geom_bar() +
  labs(x="stroke or !stroke", y = "n")
  
  
```


# Data Cleaning

```{r}
summary(d.stroke)
```


The variable **id** does not add any value to our models. So it is useless information and can be neglected.
```{r}
d.stroke <- subset(d.stroke, select=-id)
```


All variable names are written in lowercase except **Residence_type**. Let us keep the convention and rename this variable.
```{r}
d.stroke <- d.stroke %>% 
  rename(residence_type = Residence_type)
```


Checking the gender categories reveals three factors: Female, Male and Other. As there is only one observation with the factor **Other**, we should simplify our model and remove this observation. As this is a medical data set, the biological gender is higher vlaued than the identified gender. this by all means should not be taken as offense against non-binary people. 
```{r}
d.stroke <- d.stroke[c(d.stroke$gender != "Other"),]
```


We have only missing values for the variable **bmi**. As we have only 201 missing values compared to total 5110 observations it would be reasonable to drop these observations.
```{r}
mean(as.numeric(as.character(d.stroke$bmi)), na.rm = TRUE)

d.stroke <- d.stroke[d.stroke$bmi != "N/A",]
```


The remaining values for variable **bmi** are initially interpreted as factors. Let us change these to numeric values.
```{r}
d.stroke$bmi <- as.numeric(levels(d.stroke$bmi))[d.stroke$bmi]
```


```{r}
summary(d.stroke)
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$bmi, main="Histogram of BMI")
d.stroke$bmi_norm <- log(d.stroke$bmi)/max(log(d.stroke$bmi))
hist(d.stroke$bmi_norm, main="Histogram of log(BMI)")
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$age)
d.stroke$age_norm <- d.stroke$age/max(d.stroke$age)
hist(d.stroke$age_norm)
```

```{r, fig.show="hold", out.width="50%"}
hist(d.stroke$avg_glucose_level)
d.stroke$avg_glucose_level_norm <- d.stroke$avg_glucose_level/max(d.stroke$avg_glucose_level)
hist(d.stroke$avg_glucose_level_norm)
```

Datatypes:
```{r}
#d.stroke$gender <- as.factor(d.stroke$gender)
#d.stroke$hypertension <- as.factor(d.stroke$hypertension)
#d.stroke$heart_disease <- as.factor(d.stroke$heart_disease)
#d.stroke$ever_married <- as.factor(d.stroke$ever_married)
#d.stroke$work_type <- as.factor(d.stroke$work_type)
#d.stroke$residence_type <- as.factor(d.stroke$residence_type)
#d.stroke$smoking_status <- as.factor(d.stroke$smoking_status)
d.stroke$stroke <- as.factor(d.stroke$stroke)
str(d.stroke)
```

\newpage
# Graphical Analysis
```{r}
qplot(y = bmi, x = age,
      data = d.stroke,
      facets = ~ gender,
      col = as.factor(stroke))
```

```{r}
boxplot(age ~ stroke, data = d.stroke,
        main = "Influence of Age on Stroke Probability",
        ylab = "age")
```

```{r}
boxplot(avg_glucose_level ~ stroke, data = d.stroke,
        main = "Influence of Glucose Level on Stroke Probability",
        ylab = "avg_glucose_level")
```

```{r}
boxplot(bmi ~ stroke, data = d.stroke,
        main = "Influence of BMI on Stroke Probability",
        ylab = "bmi")
```

```{r}
boxplot(age ~ smoking_status, data = d.stroke,
        main = "Age vs. Smoking Status",
        ylab = "age")
```


```{r}
plot(d.stroke$bmi_norm, d.stroke$avg_glucose_level_norm, col="cornflowerblue", pch=20)
points(d.stroke$bmi_norm[d.stroke$stroke==1], d.stroke$avg_glucose_level_norm[d.stroke$stroke==1], col="firebrick", pch=20)
```


\newpage
# Train / Test / Validation Split
As we have an unbalanced data set, we must first apply some balancing algorithm. For a first approach, oversampling should be sufficient. However, to make sure, that we do not induce any errors with the oversampling algorithm, the first thing we should do is to split off some validation data. After this, oversampling can be applied to the remaining data set. For training the model, we can split now the over-sampled set into training an testing data. 

1. Split data into training and validation set (train:90% / valid:10%)
2. Apply Oversampling to the training data set
3. Split training data set into training and testing set (train:80% / test:20%)

```{r}
table(d.stroke$stroke)
```


## Validation Split
```{r}
set.seed(42)
split1<- sample(c(rep(0, 0.9 * nrow(d.stroke)), rep(1, 0.1 * nrow(d.stroke))))
table(split1)
```

```{r}
d.train <- d.stroke[split1 == 0, ]
d.valid <- d.stroke[split1== 1, ]
dim(d.train)
dim(d.valid)
```


## Oversampling
```{r}
#install.packages("ROSE")
library(ROSE)
```

```{r}
table(d.train$stroke)
```

```{r}
set.seed(42)
d.train <- ovun.sample(stroke ~ ., data=d.train, method="over", N=2*4233)$data
table(d.train$stroke)
```

```{r}
str(d.train)
```


## Train / Test Split
```{r}
set.seed(42)
split2<- sample(c(rep(0, 0.8 * nrow(d.train)), rep(1, 0.2 * nrow(d.train))))
d.test <- d.train[split2== 1, ]
d.train <- d.train[split2 == 0, ]
dim(d.train)
dim(d.test)
```


\newpage
# 1. Linear Model

```{r}
#d.lm <- lm(stroke ~ gender + age + avg_glucose_level + log(bmi), data=d.train)
#summary(d.lm)
```

```{r}
#d.lm <- lm(stroke ~ age + gender * age, data=d.train, )
#summary(d.lm)
```

```{r}
#d.lm <- lm(stroke ~ . - id, data=d.train, )
#summary(d.lm)
```

```{r}
#confint(d.lm)
```


\newpage
# 2. Generalised Linear Model with family set to Poisson.
GLM with Poisson family does not make sense on our Stroke prediction, as Poisson distribution is used for modeling Count data response variable.Stroke is binary data, so Poisson distribution does not work.
Therefore we use a glm with a binomial distribution to fit the data, as described in the next section.



\newpage
# 3. Generalised Linear Model with family set to Binomial


```{r}
d.glm <- glm(stroke ~ ., data=d.train, family="binomial")
summary(d.glm)
```

```{r}
d.glm <- glm(stroke ~ bmi_norm + age_norm + avg_glucose_level_norm , data=d.train, family="binomial")
summary(d.glm)
```

```{r}
head(predict(d.glm, d.test))
```



\newpage
# 4. Generalised Additive Model
```{r}
library("mgcv")

# ggplot 
# d.train %>%
#   ggplot(mapping = aes(y = stroke,
#                      x = bmi)) +
# geom_point() +
# geom_smooth()

# d.gam <- d.train %>%
#   gam(data = ., stroke ~ bmi_norm + avg_glucose_level_norm)
# 
# d.gam

```

\newpage
# 5. Support Vector Machine

```{r}
library(e1071)
library(caret)
```

For the training of the SVM we use the normalized values of avg_glucose_level, bmi, and age. Therefore, we drop the columns where these values are not normalized. We choose the kernel as linear and a cost value of 5, as these values returned a reasonable result. In the chapter *7.* we investigate different settings and models with the use of cross validation. We have here a SVM with two classes 1:"stroke" and 0:"no stroke" and a total of 3529 support vectors, which seems to be a lot, comparing to the examples covered in class. 


```{r}
set.seed(42)
d.svm <- svm(stroke ~ . - avg_glucose_level - bmi - age, data=d.train, kernel = "linear", type="C-classification", cost = 5)
summary(d.svm)
```

If we predict the values for the train data set we get an overall accuracy of 77.6%. This value is relatively low in comparison, if the model would always just return "no stroke", the accuracy would be at 95%. However, the sensitivity is at 81.5% which means, that we are able to predict most of the positive cases.

```{r}
test_pred <- predict(d.svm, d.test)
conf_matrix <- confusionMatrix(as.factor(test_pred), d.test$stroke, positive="1")
conf_matrix
```

The problem with our training data is, that we have used oversampling for balancing the data set. Therefore, the test data set also contains values from the train data set and this means, that our algorithm might have seen the test data during the training period. It is therefore important to compare check the model with a separate validation set. With the validation data set the accuracy is almost at the same level as with the training data - 74.3%. However, the sensitivity is here a bit higher with 91.7% - this might just be luck and will be checked in chapter 7.

```{r}
valid_pred <- predict(d.svm, d.valid)
conf_matrix <- confusionMatrix(as.factor(valid_pred), d.valid$stroke, positive="1")
conf_matrix
```


\newpage
# 6. Neural Network
```{r}
library(nnet)
```


```{r, results=FALSE}
set.seed(42)
d.net <- nnet(stroke ~ . - avg_glucose_level - bmi - age, data = d.train, size=20, maxit=10000, rang=0.1, decay=5e-3, MaxNWts = 20000)
d.net
```

```{r}
pred <- predict(d.net, d.test, type="class")
cm_nn <- table(pred=pred, true=d.test$stroke)
cm_nn
```

```{r}
pred <- predict(d.net, d.valid, type="class")
cm_nn <- table(pred=pred, true=d.valid$stroke)
cm_nn
```

```{r}
conf_matrix <- confusionMatrix(as.factor(pred), d.valid$stroke, positive="1")
conf_matrix
```


\newpage
# 7. Cross Validation on SVM and Neural Network

```{r, echo=FALSE}
sens.svm.lin <- c()
sens.svm.rad <- c()
sens.nnet.10 <- c()
sens.nnet.15 <- c()

set.seed(42)
for(i in 1:3){
  
  ## 1) Create train test split
  split1 <- sample(c(rep(0, 0.8 * nrow(d.stroke)), rep(1, 0.1 * nrow(d.stroke))))
  data.train <- d.stroke[split1 == 0, ]
  data.test <- d.stroke[split1== 1, ]
  
  ## 2) Apply oversampling
  data.train <- ovun.sample(stroke ~ ., data=data.train, method="over", N=2*nrow(data.train))$data
  
  ## 3) Train SVM
  model.svm.lin <- svm(stroke ~ . - avg_glucose_level - bmi - age, data=d.train, kernel = "linear", type="C-classification", cost = 5)
  model.svm.rad <- svm(stroke ~ . - avg_glucose_level - bmi - age, data=d.train, kernel = "radial", type="C-classification", cost = 5)
  
  ## 3) Train neuronal network
  model.nnet.10 <- nnet(stroke ~ . - avg_glucose_level - bmi - age, data = data.train, size=10, maxit=10000, rang=0.1, decay=5e-3, MaxNWts = 20000, trace=FALSE)
  model.nnet.15 <- nnet(stroke ~ . - avg_glucose_level - bmi - age, data = data.train, size=15, maxit=10000, rang=0.1, decay=5e-3, MaxNWts = 20000, trace=FALSE)
  
  ## 4) Prediction on test data
  model.svm.lin.pred <- predict(model.svm.lin, data.test, type="class")
  model.svm.rad.pred <- predict(model.svm.rad, data.test, type="class")
  model.nnet.10.pred <- predict(model.nnet.10, data.test, type="class")
  model.nnet.15.pred <- predict(model.nnet.15, data.test, type="class")
  
  ## 5) Evaluation
  conf_matrix <- confusionMatrix(as.factor(model.svm.lin.pred), data.test$stroke, positive="1")
  sens.svm.lin[i] <- conf_matrix$byClass["Sensitivity"]
  
  conf_matrix <- confusionMatrix(as.factor(model.svm.rad.pred), data.test$stroke, positive="1")
  sens.svm.rad[i] <- conf_matrix$byClass["Sensitivity"]
  
  conf_matrix <- confusionMatrix(as.factor(model.nnet.10.pred), data.test$stroke, positive="1")
  sens.nnet.10[i] <- conf_matrix$byClass["Sensitivity"]
  
  conf_matrix <- confusionMatrix(as.factor(model.nnet.15.pred), data.test$stroke, positive="1")
  sens.nnet.15[i] <- conf_matrix$byClass["Sensitivity"]
}
```


```{r}
sens.svm.lin
sens.svm.rad
sens.nnet.10
sens.nnet.15
```


















